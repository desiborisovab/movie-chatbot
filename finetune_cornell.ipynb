{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a11cf6d-6aae-4e1a-b3fa-051564d8a736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d367f08f-b987-41f1-823c-505ae6985ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "RUN_ID = \"fe9ecda0ef3d49d0ae5b96847f975de9\"\n",
    "\n",
    "# 1) Load model from MLflow\n",
    "model_uri = f\"runs:/{RUN_ID}/model\"\n",
    "lm_model = mlflow.tensorflow.load_model(model_uri)\n",
    "\n",
    "# 2) Load vocab.json artifact from MLflow\n",
    "vocab_path = mlflow.artifacts.download_artifacts(\n",
    "    run_id=RUN_ID,\n",
    "    artifact_path=\"vocab.json\"\n",
    ")\n",
    "\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(\"Loaded from MLflow. vocab_size =\", len(vocab))\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=len(vocab),\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "vectorizer.set_vocabulary(vocab)\n",
    "\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 128\n",
    "print(\"Loaded model + vocab_size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5624e5-d010-433b-8821-385256b87c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "lines_df = spark.table(\"workspace.default.movie_lines\")\n",
    "\n",
    "# Your table has one column called \"value\"\n",
    "line_col = \"value\"\n",
    "\n",
    "# Split: L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
    "parts = F.split(F.col(line_col), \" \\\\+\\\\+\\\\+\\\\$\\\\+\\\\+\\\\+ \")\n",
    "\n",
    "line_map_df = (lines_df\n",
    "    .select(\n",
    "        parts.getItem(0).alias(\"line_id\"),\n",
    "        parts.getItem(4).alias(\"text\")\n",
    "    )\n",
    "    .where(F.col(\"line_id\").isNotNull())\n",
    "    .where(F.col(\"text\").isNotNull())\n",
    "    .where(F.length(F.col(\"text\")) > 0)\n",
    ")\n",
    "\n",
    "print(\"Lines rows:\", line_map_df.count())\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load conversations\n",
    "# ---------------------------\n",
    "convs_df = spark.table(\"workspace.default.movie_conversations\")\n",
    "conv_col = \"value\"\n",
    "\n",
    "conv_parts = F.split(F.col(conv_col), \" \\\\+\\\\+\\\\+\\\\$\\\\+\\\\+\\\\+ \")\n",
    "ids_field = conv_parts.getItem(3)  # \"['L194','L195',...]\"\n",
    "\n",
    "conv_ids_df = (convs_df\n",
    "    .select(ids_field.alias(\"ids_field\"))\n",
    "    .withColumn(\"ids\", F.expr(\"regexp_extract_all(ids_field, 'L\\\\\\\\d+', 0)\"))\n",
    "    .where(F.size(\"ids\") >= 2)\n",
    ")\n",
    "\n",
    "print(\"Conversations:\", conv_ids_df.count())\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Explode conversations into turns\n",
    "# ---------------------------\n",
    "exploded = conv_ids_df.select(\n",
    "    F.monotonically_increasing_id().alias(\"conv_id\"),\n",
    "    F.posexplode(\"ids\").alias(\"pos\", \"line_id\")\n",
    ")\n",
    "\n",
    "# Join with actual text\n",
    "joined = exploded.join(line_map_df, on=\"line_id\", how=\"left\")\n",
    "\n",
    "# Assign User/Bot roles\n",
    "with_roles = joined.withColumn(\n",
    "    \"turn\",\n",
    "    F.when(F.col(\"pos\") % 2 == 0,\n",
    "           F.concat(F.lit(\"User: \"), F.col(\"text\")))\n",
    "     .otherwise(\n",
    "           F.concat(F.lit(\"Bot: \"), F.col(\"text\")))\n",
    ")\n",
    "\n",
    "# Limit turns per conversation (10)\n",
    "with_roles_limited = with_roles.where(F.col(\"pos\") < 10)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build chat documents\n",
    "# ---------------------------\n",
    "chat_docs_df = (with_roles_limited\n",
    "    .groupBy(\"conv_id\")\n",
    "    .agg(F.concat_ws(\"\\n\", F.collect_list(\"turn\")).alias(\"chat_doc\"))\n",
    "    .where(F.length(\"chat_doc\") > 0)\n",
    ")\n",
    "\n",
    "display(chat_docs_df.limit(5))\n",
    "\n",
    "# ---------------------------\n",
    "# 5. SAFE COLLECT (no UTF-8 crash)\n",
    "# ---------------------------\n",
    "chat_docs_b64_df = chat_docs_df.select(\n",
    "    F.base64(F.encode(F.col(\"chat_doc\"), \"UTF-8\")).alias(\"chat_doc_b64\")\n",
    ")\n",
    "\n",
    "chat_docs_b64 = [\n",
    "    r[\"chat_doc_b64\"]\n",
    "    for r in chat_docs_b64_df.limit(50000).collect()\n",
    "]\n",
    "\n",
    "chat_docs = [\n",
    "    base64.b64decode(s).decode(\"utf-8\", errors=\"replace\")\n",
    "    for s in chat_docs_b64\n",
    "]\n",
    "\n",
    "print(\"chat_docs:\", len(chat_docs))\n",
    "print(chat_docs[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e038820-d9b5-4bbc-96f1-17209af1713e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_windows(token_ids):\n",
    "    return tf.data.Dataset.from_tensor_slices(token_ids).window(\n",
    "        SEQ_LEN + 1, shift=SEQ_LEN, drop_remainder=True\n",
    "    ).flat_map(lambda w: w.batch(SEQ_LEN + 1))\n",
    "\n",
    "def split_xy(seq):\n",
    "    return seq[:-1], seq[1:]\n",
    "\n",
    "def doc_to_ds(doc):\n",
    "    ids = vectorizer(tf.expand_dims(doc, 0))[0]\n",
    "    ids = tf.boolean_mask(ids, ids > 0)\n",
    "    return make_windows(ids).map(split_xy, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(chat_docs).shuffle(20000, seed=42)\n",
    "lm_ds = text_ds.flat_map(doc_to_ds).shuffle(20000, seed=42)\n",
    "\n",
    "VAL_EXAMPLES = 2000\n",
    "val_ds = lm_ds.take(VAL_EXAMPLES).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_ds = lm_ds.skip(VAL_EXAMPLES).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds_rep = train_ds.repeat()\n",
    "val_ds_rep   = val_ds.repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5deb7821-06ad-4cb6-acd3-57f4cdf8ce84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 200\n",
    "VAL_STEPS = 20\n",
    "\n",
    "history = lm_model.fit(\n",
    "    train_ds_rep,\n",
    "    validation_data=val_ds_rep,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabc384e-2f14-4e30-8cbc-654532c38d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow, mlflow.tensorflow, json\n",
    "\n",
    "mlflow.set_experiment(\"/Users/desiborisovab@gmail.com/movie_chatbot_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"finetune_cornell_dialogue\") as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.tensorflow.log_model(lm_model, artifact_path=\"model\")\n",
    "    mlflow.log_text(json.dumps(vocab), \"vocab.json\")\n",
    "\n",
    "    mlflow.log_param(\"finetune_dataset\", \"cornell\")\n",
    "    mlflow.log_param(\"SEQ_LEN\", SEQ_LEN)\n",
    "    mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"STEPS_PER_EPOCH\", STEPS_PER_EPOCH)\n",
    "    mlflow.log_param(\"VAL_STEPS\", VAL_STEPS)\n",
    "\n",
    "print(\"Fine-tuned model logged. run_id =\", run_id)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "finetune_cornell",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
