{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850223a2-cd75-4bb9-b470-0d79c6c48208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528995b4-65f1-49ff-83fc-8fccfe0993ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6f4179-6a3e-4e0f-ad60-80ee2968fe1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tf:\", tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "print(\"tf.keras OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0f028d-8cc3-43f5-bf5c-3603e01919e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#load the table\n",
    "df = spark.table(\"default.wiki_movie_plots_deduped\").toPandas()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "# clean the data\n",
    "def clean_text(s):\n",
    "    #remove missing values\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    #force to a string and replace with non breaking space\n",
    "    s = str(s).replace(\"\\u00a0\", \" \")\n",
    "    # collaps multiple spaces, newtabs, newlines\n",
    "    # remove leading and trailing space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def safe_year(y):\n",
    "    # extract 4 digit year or return empty\n",
    "    try:\n",
    "        return str(int(y))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def take_cast(cast, n=5):\n",
    "    # clean the cast string\n",
    "    cast = clean_text(cast)\n",
    "    # if empty, return empty\n",
    "    if not cast:\n",
    "        return \"\"\n",
    "    # split into names\n",
    "    # return the first 5 names\n",
    "    parts = [p.strip() for p in cast.split(\",\") if p.strip()]\n",
    "    return \", \".join(parts[:n])\n",
    "\n",
    "# each movie(row) to a strucutred text block\n",
    "def row_to_doc(r):\n",
    "    title    = clean_text(r[\"Title\"])\n",
    "    year     = safe_year(r[\"Release Year\"])\n",
    "    origin   = clean_text(r[\"Origin/Ethnicity\"])\n",
    "    director = clean_text(r[\"Director\"])\n",
    "    genre    = clean_text(r[\"Genre\"])\n",
    "    cast     = take_cast(r[\"Cast\"], n=5)\n",
    "    plot     = clean_text(r[\"Plot\"])\n",
    "\n",
    "    if not title or not plot:\n",
    "        return \"\"\n",
    "\n",
    "    return (\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Year: {year}\\n\"\n",
    "        f\"Origin: {origin}\\n\"\n",
    "        f\"Director: {director}\\n\"\n",
    "        f\"Cast: {cast}\\n\"\n",
    "        f\"Genre: {genre}\\n\"\n",
    "        f\"Plot: {plot}\\n\"\n",
    "    ).strip()\n",
    "\n",
    "docs = [row_to_doc(r) for _, r in df.iterrows()]\n",
    "docs = [d for d in docs if d]\n",
    "\n",
    "# print(\"Documents:\", len(docs))\n",
    "# print(\"\\n--- SAMPLE DOCUMENT ---\\n\")\n",
    "# print(docs[:3][:500])\n",
    "\n",
    "# train unsupervised\n",
    "# max number of unique tokens\n",
    "VOCAB_SIZE = 20000\n",
    "# token seq length\n",
    "SEQ_LEN = 128\n",
    "# size per gradient update\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# convert to TensorFlow dataset and shuffle it\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(docs).shuffle(10000, seed=42)\n",
    "\n",
    "#for x in text_ds.take(3):\n",
    "#    print(x)\n",
    "\n",
    "# tokenizer\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "vectorizer.adapt(text_ds.batch(256))\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Vocab sample:\", vocab[:10])\n",
    "\n",
    "def make_windows(token_ids):\n",
    "    return tf.data.Dataset.from_tensor_slices(token_ids).window(\n",
    "        SEQ_LEN + 1, shift=SEQ_LEN, drop_remainder=True\n",
    "    ).flat_map(lambda w: w.batch(SEQ_LEN + 1))\n",
    "\n",
    "def split_xy(seq):\n",
    "    return seq[:-1], seq[1:]\n",
    "\n",
    "def doc_to_ds(doc):\n",
    "    ids = vectorizer(tf.expand_dims(doc, 0))[0]\n",
    "    ids = tf.boolean_mask(ids, ids > 0)\n",
    "    return make_windows(ids).map(split_xy, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "lm_ds = text_ds.shuffle(5000, seed=42).flat_map(doc_to_ds).shuffle(20000, seed=42)\n",
    "\n",
    "VAL_EXAMPLES = 5000\n",
    "\n",
    "val_ds = (lm_ds.take(VAL_EXAMPLES)\n",
    "          .batch(BATCH_SIZE)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds = (lm_ds.skip(VAL_EXAMPLES)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds_rep = train_ds.repeat()\n",
    "val_ds_rep   = val_ds.repeat()\n",
    "\n",
    "# Pick explicit steps since cardinality is unknown (-2)\n",
    "STEPS_PER_EPOCH = 500\n",
    "VAL_STEPS = 50\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "inputs = layers.Input(shape=(SEQ_LEN,), dtype=tf.int32)\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=256)(inputs)\n",
    "# Must keep return_sequences=True so attention can see all time steps\n",
    "lstm_out = layers.LSTM(512, return_sequences=True)(x)\n",
    "# Self-attention over the LSTM outputs (query=keys=values=lstm_out)\n",
    "attn_out = layers.Attention()([lstm_out, lstm_out])\n",
    "# Combine original LSTM signal + attended context\n",
    "x = layers.Concatenate()([lstm_out, attn_out])\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# Next-token logits at every timestep\n",
    "outputs = layers.Dense(vocab_size)(x)\n",
    "lm_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "lm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "history = lm_model.fit(\n",
    "    train_ds_rep,\n",
    "    validation_data=val_ds_rep,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import json\n",
    "\n",
    "mlflow.set_experiment(\"/Users/desiborisovab@gmail.com/movie_chatbot_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"lstm_attention_movie_lm\") as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.tensorflow.log_model(lm_model, artifact_path=\"model\")\n",
    "    mlflow.log_text(json.dumps(vocab), \"vocab.json\")\n",
    "\n",
    "    mlflow.log_param(\"SEQ_LEN\", SEQ_LEN)\n",
    "    mlflow.log_param(\"VOCAB_SIZE\", len(vocab))\n",
    "    mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"STEPS_PER_EPOCH\", STEPS_PER_EPOCH)\n",
    "    mlflow.log_param(\"VAL_STEPS\", VAL_STEPS)\n",
    "\n",
    "print(\"Logged to MLflow. run_id =\", run_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "movie_chatbot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
